{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "imp3WcLQQUTc"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "titanic = pd.read_csv(r'C:\\Users\\Admin\\Desktop\\titanic.csv')\n",
        "titanic.info()\n",
        "print(titanic.describe())\n",
        "print(titanic.columns)\n",
        "\n",
        "#ax = sns.countplot(x = 'Pclass', hue = 'Survived', palette = 'Set1', data = titanic)\n",
        "#ax.set(title = 'Passenger status (Survived/Died) against Passenger Class',xlabel = 'Passenger Class', ylabel = 'Total')\n",
        "#w=sns.countplot(x='Sex',hue='Survived',palette = 'Set1',data=titanic)\n",
        "\n",
        "#sns.heatmap(titanic.isnull(), cbar=False)\n",
        "#interval = (0,20,30,60,120)\n",
        "#categories = ['Children','Teens','Adult', 'Old']\n",
        "#titanic['Age_cats'] = pd.cut(titanic.Age, interval, labels = categories)\n",
        "\n",
        "#ax = sns.countplot(x = 'Age_cats',  data = titanic, hue = 'Survived', palette = 'Set1')\n",
        "#titanic[\"Age\"].plot(kind='box')\n",
        "titanic[\"Age\"]=titanic[\"Age\"][titanic[\"Age\"]<65]\n",
        "#titanic[\"Age\"].plot(kind='box')\n",
        "\n",
        "from sklearn.impute import KNNImputer\n",
        "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
        "X=titanic[\"Age\"].to_numpy().copy().reshape(-1,1)\n",
        "X_t=imputer.fit_transform(X)\n",
        "titanic[\"Age\"]=X_t\n",
        "\n",
        "\n",
        "\n",
        "titanic['Title'] = titanic['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
        "print(titanic[\"Title\"].value_counts())\n",
        "title_mapping = {\"Mr\": 0, \"Miss\": 1, \"Mrs\": 2,\n",
        "                 \"Master\": 3, \"Dr\": 3, \"Rev\": 3, \"Col\": 3, \"Major\": 3, \"Mlle\": 3,\"Countess\": 3,\n",
        "                 \"Ms\": 3, \"Lady\": 3, \"Jonkheer\": 3, \"Don\": 3, \"Dona\" : 3, \"Mme\": 3,\"Capt\": 3,\"Sir\": 3 }\n",
        "titanic['Title'] = titanic['Title'].map(title_mapping)\n",
        "titanic[\"FamilySize\"] = titanic[\"SibSp\"] + titanic[\"Parch\"] + 1\n",
        "titanic[\"Sex\"]=titanic['Sex'].map(dict(zip(['male', 'female'], [0.0, 1.0])))\n",
        "titanic[\"Embarked\"]=titanic['Embarked'].map(dict(zip(['S', 'C','Q'], [0.0, 1.0,2.0])))\n",
        "\n",
        "\n",
        "titanic=titanic.drop(columns=[\"Ticket\",\"Cabin\",\"Name\",\"PassengerId\"])\n",
        "\n",
        "titanic['Embarked'].fillna(titanic['Embarked'].mode()[0], inplace = True)\n",
        "\n",
        "sns.heatmap(titanic.corr());\n",
        "\n",
        "#plt.show()\n",
        "titanic.info()\n",
        "from sklearn.model_selection import train_test_split\n",
        "X=titanic.copy()\n",
        "X=X.drop(columns=\"Survived\").to_numpy().astype(float)\n",
        "Y=titanic['Survived'].to_numpy().astype(float)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle=True, random_state=1)\n",
        "##Data normalization is used in machine learning to make model training less sensitive to the scale of features. This allows our model to converge to better weights and, in turn, leads to a more accurate model.\n",
        "##StandardScaler removes the mean and scales each feature/variable to unit variance. This operation is performed feature-wise in an independent way.\n",
        "from sklearn import preprocessing\n",
        "X_normalized = preprocessing.normalize(X, norm='l2')\n",
        "\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "kf = KFold(n_splits=7, random_state=0, shuffle=True)\n",
        "from sklearn.linear_model import SGDClassifier, RidgeClassifierCV, Perceptron\n",
        "\n",
        "model = SGDClassifier()\n",
        "for train_index, valid_index in kf.split(X):\n",
        "        x_train, x_valid = X[train_index], X[valid_index]\n",
        "        y_train, y_valid = Y[train_index], Y[valid_index]\n",
        "        model.fit(x_train,y_train)\n",
        "\n",
        "        y_pred = model.predict(x_valid)\n",
        "res = {\"Accuracy_V\":[] , 'Precision_V':[] , 'Recall_V':[] , 'F1-Score_V':[]}\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import prettytable\n",
        "res['Accuracy_V'].append(accuracy_score(y_valid , y_pred))\n",
        "res['Precision_V'].append(precision_score(y_valid , y_pred , average = 'macro'))\n",
        "res['Recall_V'].append(recall_score(y_valid , y_pred , average = 'macro'))\n",
        "res['F1-Score_V'].append(f1_score(y_valid , y_pred , average = 'macro'))\n",
        "\n",
        "table = prettytable.PrettyTable(['key', 'value'])\n",
        "for key, item in res.items():\n",
        "    table.add_row([key, sum(item) / len(item)])\n",
        "print(res,table,model)\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors = 3)\n",
        "knn.fit(x_train, y_train)\n",
        "Y_pred = knn.predict(x_test)\n",
        "acc_knn = round(knn.score(x_train, y_train) * 100, 2)\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "decision_tree = DecisionTreeClassifier()\n",
        "decision_tree.fit(x_train, y_train)\n",
        "Y_pred = decision_tree.predict(x_test)\n",
        "acc_decision_tree = round(decision_tree.score(x_train, y_train) * 100, 2)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(x_train, y_train)\n",
        "Y_pred = logreg.predict(x_test)\n",
        "acc_log = round(logreg.score(x_train, y_train) * 100, 2)\n",
        "acc_log\n",
        "\n",
        "gaussian = GaussianNB()\n",
        "gaussian.fit(x_train, y_train)\n",
        "Y_pred = gaussian.predict(x_test)\n",
        "acc_gaussian = round(gaussian.score(x_train, y_train) * 100, 2)\n",
        "\n",
        "random_forest = RandomForestClassifier(n_estimators=100)\n",
        "random_forest.fit(x_train, y_train)\n",
        "Y_pred = random_forest.predict(x_test)\n",
        "random_forest.score(x_train, y_train)\n",
        "acc_random_forest = round(random_forest.score(x_train, y_train) * 100, 2)\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "svc = SVC()\n",
        "svc.fit(x_train, y_train)\n",
        "Y_pred = svc.predict(x_test)\n",
        "acc_svc = round(svc.score(x_train, y_train) * 100, 2)\n",
        "\n",
        "Ridge= RidgeClassifierCV()\n",
        "Ridge.fit(x_train, y_train)\n",
        "Y_pred = Ridge.predict(x_test)\n",
        "acc_Ridge= round(Ridge.score(x_train, y_train) * 100, 2)\n",
        "print(acc_Ridge)\n",
        "\n",
        "perceptron = Perceptron()\n",
        "perceptron.fit(x_train, y_train)\n",
        "Y_pred = perceptron.predict(x_test)\n",
        "acc_perceptron = round(perceptron.score(x_train, y_train) * 100, 2)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}